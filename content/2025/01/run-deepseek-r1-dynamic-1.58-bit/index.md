---
title: Run DeepSeek-R1 Dynamic 1.58-bit
date: 2025-01-28
extra:
  source: https://unsloth.ai/blog/deepseekr1-dynamic
  original_title: Run DeepSeek-R1 Dynamic 1.58-bit
---
## Summary
**æ‘˜è¦**ï¼š
æœ¬æ–‡ç”±Daniel & Michaelæ’°å†™ï¼Œæ ‡é¢˜ä¸ºã€ŠRun DeepSeek R1 Dynamic 1.58-bitã€‹ï¼Œä»‹ç»äº†ä½œè€…å¦‚ä½•å°†DeepSeek-R1çš„å‚æ•°æ¨¡å‹å¤§å°ä»720GBå‰Šå‡åˆ°131GBï¼ŒåŒæ—¶ä¿æŒéå¸¸åˆç†çš„æ€§èƒ½ã€‚é€šè¿‡åŠ¨æ€é‡åŒ–ç‰¹å®šå±‚ä¸ºæ›´é«˜ä½æ•°ï¼ˆå¦‚4ä½ï¼‰ï¼ŒåŒæ—¶è®©å¤§éƒ¨åˆ†MoEå±‚ä¿æŒåœ¨1.5ä½ï¼Œä½œè€…æˆåŠŸåœ¨GPUï¼ˆå¦‚2xH100 80GBï¼‰ä¸Šå®ç°å¿«é€Ÿæ¨ç†å¹¶è¾¾åˆ°çº¦æ¯ç§’140ä¸ªä»¤ç‰Œçš„æ€§èƒ½ã€‚æä¾›äº†ä¸€ä¸ªé«˜æ€§ä»·æ¯”çš„æ¨¡å‹ç‰ˆæœ¬ï¼Œå¹¶åœ¨æ¨¡å‹å®˜ç½‘ä¸Šå…è´¹ä¾›ç”¨æˆ·ä¸‹è½½ã€‚å› æ­¤ï¼Œè¿™ç¯‡åšæ–‡ä¸»è¦å…³æ³¨çš„æ¨¡å‹è¯¦ç»†å‚æ•°ã€æ€§èƒ½æµ‹è¯•ã€é‡åŒ–æ–¹æ³•ã€æ¨¡å‹æ„å»ºåŠéƒ¨ç½²æŒ‡å—ï¼Œä»¥åŠä¸€ä¸ªç”¨äºéªŒè¯æ¨¡å‹åŠŸèƒ½çš„ç¤ºä¾‹æ¸¸æˆã€‚

**è¦ç‚¹æ€»ç»“**ï¼š
- **æ¨¡å‹å‹ç¼©**ï¼šé€šè¿‡åŠ¨æ€é‡åŒ–æŠ€æœ¯ï¼ŒDeepSeek-R1æ¨¡å‹å¤§å°æ˜¾è‘—ç¼©å‡è‡³131GBï¼ŒåŒæ—¶ä¿æŒé«˜ç²¾åº¦æ€§èƒ½ã€‚
- **åŠ é€Ÿæ¨ç†**ï¼šæ¨¡å‹åœ¨160GB VRAMåœºæ™¯ä¸‹å¯å®ç°å¿«é€Ÿæ¨ç†ï¼Œé€Ÿåº¦å¯è¾¾æ¯ç§’140ä¸ªä»¤ç‰Œã€‚
- **å¤šç§é‡åŒ–ç‰ˆæœ¬**ï¼šæä¾›äº†ä¸åŒå¤§å°ï¼ˆ131GBåˆ°212GBï¼‰çš„åŠ¨æ€é‡åŒ–ç‰ˆæœ¬ä»¥é€‚åº”ä¸åŒç¡¬ä»¶ç¯å¢ƒï¼Œç»‘å®šå™¨æ”¯æŒåŒ…æ‹¬Ollamaã€vLLMç­‰ã€‚
- **æ€§èƒ½è¯„ä¼°**ï¼šé€šè¿‡Flappy Birdæ¸¸æˆä»»åŠ¡å¯¹æ¨¡å‹çš„é‡åŒ–ç‰ˆæœ¬è¿›è¡Œè¯„ä¼°ï¼Œå¹¶ä¸åŸå§‹æ¨¡å‹æ€§èƒ½è¿›è¡Œäº†å¯¹æ¯”ï¼Œè¡¨ç°å‡ºä¼˜è‰¯çš„ç¨³å®šæ€§åŠåˆ›æ„è¾“å‡ºã€‚
- **æ¨¡å‹ä½¿ç”¨æŒ‡å—**ï¼šæä¾›ä»å®‰è£…åˆ°è¿è¡Œå’Œéƒ¨ç½²çš„å®Œæ•´æŒ‡å¼•ï¼ŒåŒ…æ‹¬GPUåˆ©ç”¨ç­–ç•¥åŠä¸åŒå¤§å°æ¨¡å‹åœ¨ä¸åŒç¡¬ä»¶æ¡ä»¶ä¸‹æœ€ä½³é…ç½®ã€‚

å®‰æ’æ¸…æ™°ã€å…¨é¢çš„éƒ¨ç½²æµç¨‹ï¼Œé…åˆæ·±å…¥çš„æŠ€æœ¯è§£æï¼Œä½¿ä¸åŒèµ„æºæ¡ä»¶çš„ç”¨æˆ·å‡èƒ½é«˜æ•ˆåˆ©ç”¨DeepSeek-R1çš„åŠ¨æ€1.58ä½ç‰ˆæœ¬ã€‚
## Full Content
Title: Run DeepSeek-R1 Dynamic 1.58-bit

URL Source: https://unsloth.ai/blog/deepseekr1-dynamic

Markdown Content:
Run DeepSeek R1  
Dynamic 1.58-bit

Jan 27, 2025 â€¢ By Daniel & Michael
----------------------------------

Jan 27, 2025
------------

â€¢
-

By Daniel & Michael
-------------------

DeepSeek-R1 has been making waves recently by rivaling OpenAI's O1 reasoning model while being fully open-source. We explored how to enable more local users to run it and managed to quantize DeepSeekâ€™s R1 671B parameter model to 131GB in size, a **80% reduction** in size from the original 720GB, whilst being very functional.By studying DeepSeek R1â€™s architecture, we managed to selectively quantize certain layers to higher bits (like 4bit), and leave most MoE layers (like those used in GPT-4) to 1.5bit (see [Unsloth Dynamic 4-bit](https://unsloth.ai/blog/dynamic-4bit)). Naively quantizing all layers breaks the model entirely, causing endless loops and gibberish outputs. Our dynamic quants solve this.

The 1.58bit quantization should fit in 160GB of VRAM for fast inference (2x H100 80GB), with it attaining around **140 tokens per second**. You don't need VRAM (GPU) to run 1.58bit R1, just 20GB of RAM (CPU) will work however it maybe slow. For optimal performance, we recommend the sum of VRAM + RAM to be at least 80GB+.

We uploaded dynamic quantized versions ranging from 131GB to 212GB in size to: [huggingface.co/unsloth/DeepSeek-R1-GGUF](https://huggingface.co/unsloth/DeepSeek-R1-GGUF)

P.S. if you liked our work, feel free to â­Star us: [github.com/unslothai/unsloth](https://github.com/unslothai/unsloth) or follow us on X [@UnslothAi](https://x.com/unslothai) ğŸ’–

ğŸ¦¥ 1. Dynamic Quantized versions

We provide 4 dynamic quantized versions. The first 3 uses an importance matrix to calibrate the quantization process (imatrix via llama.cpp) to allow lower bit representations. The last 212GB version is a general 2bit quant with no calibration.

These instructions work for the R1 distilled and non distilled models, however keep in mind that they will require different hardware requirements. See further below for R1 requirements.

ğŸ“Š 2. Benchmarks and ablations

To test all quantized models, instead of relying on general benchmarks, we instead ask DeepSeek r1 to create a Flappy Bird game with 3 tries (pass@3), and we score it on 10 criteria (like using random colors, random shapes, whether it can run in an Python interpreter). We used seed 3407, 3408 and 3409, and the suggested temperature of 0.6.On the left, we have an example of what [chat.deepseek.com](https://chat.deepseek.com/) generated. On the right is the 1.58bit version.

DeepSeek Original
-----------------

![Image 18](https://unsloth.ai/cgi/image/InShot_20250127_043158375_H8Uu6tyJXYAFwUEIu04Am.gif?width=1080&quality=80&format=auto)

1.58-bit Version
----------------

![Image 19](https://unsloth.ai/cgi/image/InShot_20250127_042648160_lrtL8-eRhl4qtLaUDSU87.gif?width=1080&quality=80&format=auto)

We see surprisingly that our dynamic 1.58bit version seems to still produce valid output!  
However, if you DO NOT use our dynamic 1.58bit version and instead naively quantize all layers, you will get infinite repetitions like in seed 3407: â€œColours with dark Colours with dark Colours with dark Colours with dark Colours with darkâ€ or in seed 3408: â€œSet up the Pygame's Pygame display with a Pygame's Pygame's Pygame's Pygame's Pygame's Pygame's Pygame's Pygame's Pygame'sâ€.Similarly, if you do not use our dynamic version, and instead quantize all layers to 1.75bits (149GB), infinite repetitions stop, but the result is totally incorrect. All output produces fully black screens. If you quantize all layers to even 2.06bits (175GB), the result is even worse than the 1.58bit (131GB) dynamic quant. You would rather use the 2.22bit (183GB) version which is superior in performance.

The 1.58bit dynamic quants does sometimes rarely produce 1 incorrect token per 8000 tokens, which we need to comment out. Using [min\_p = 0.1](https://arxiv.org/abs/2407.01082) or 0.05 should mitigate the 1.58bit from generating singular incorrect tokens.

For a summary for a score out of 10 and Pass@3, we get:

| Model Size | Dynamic Quant | Model Size | Basic Quant |
| --- | --- | --- | --- |
| 131GB | 6.92 | 133GB | 0 |
| 158GB | 9.08 | 149GB | 1.67 |
| 183GB | 9.17 | 175GB | 6.17 |

We provide more detailed results at the end of the blog post.

ğŸ‹ 3. Exploiting DeepSeek R1â€™s architecture

In [our previous analysis](https://x.com/danielhanchen/status/1872719599029850391) of the DeepSeek V3 model, which used DeepSeek r1 for synthetic data generation, we noted that the first 3 layers of DeepSeek are fully dense, and not MoE. As a refresher, MoE (mixture of experts) layers allow us to increase the number of parameters in a model, without increasing the number of FLOPs used, since we dynamically mask most entries as 0, and so we essentially skip doing matrix multiples on the zeroed out entries.![Image 20](https://unsloth.ai/cgi/image/deepseek_v3_analysis_gTjgng6coSIvRvrUkg3fL.jpg?width=1200&quality=80&format=auto)The goal of MoEs is to â€œtrickâ€ the scaling laws, since we increase the number of parameters without changing the compute cost. For more notes on MoEs and a new method called Memory Layers, which aims to do better than MoEs, see this tweet: [x.com/danielhanchen/status/1868748998783517093](https://x.com/danielhanchen/status/1868748998783517093)By combining 4 ideas, including:

*   Our [4-bit Dynamic Quantization](https://unsloth.ai/blog/dynamic-4bit) method
*   The [1.58-bit LLMs paper](https://arxiv.org/abs/2402.17764)
*   [Llama.cppâ€™s 1.5-bit](https://github.com/ggerganov/llama.cpp/pull/5453) quantization
*   The [Super Weights paper](https://arxiv.org/abs/2411.07191)

We managed to employ these insights:

*   The first 3 dense layers use 0.5% of all weights. Weâ€™ll leave these as 4 or 6bit.
*   MoE layers use shared experts, using 1.5% of weights. Weâ€™ll use 6bit.
*   We can leave all MLA attention modules as 4 or 6bit, using <5% of weights. We should quantize the attention output (3%), but itâ€™s best to leave it in higher precision.
*   The down\_proj is the most sensitive to quantization, especially in the first few layers. We corroborated our findings with the Super Weights paper, our dynamic quantization method and llama.cppâ€™s GGUF quantization methods. So, we shall leave the first 3 to 6 MoE down\_proj matrices in higher precision. For example in the [Super Weights paper](https://arxiv.org/pdf/2411.07191), we see nearly all weights which should NOT be quantized are in the down\_proj:  
    ![Image 21](https://unsloth.ai/cgi/image/superweights_t3PXOuE0uhMCFWT1AIBeb.png?width=2048&quality=80&format=auto)The main insight on why all the â€œsuper weightsâ€ or the most important weights are in the down\_proj is because of SwiGLU which does:\[ f â¡ ( X â¢ W g â¢ a â¢ t â¢ e ) âˆ— ( X â¢ W u â¢ p ) \] â¢ W d â¢ o â¢ w â¢ n
    
      
    This means the up and gate projection essentially multiply to form high numbers, and the down\_proj has to scale them down - this means quantizing the down\_proj might not be a good idea, especially in the early layers of the transformer.
*   We should leave the embedding and lm\_head as 4bit and 6bit respectively. The MoE router and all layer norms are left in 32bit.
*   This leaves ~88% of the weights as the MoE weights! By quantizing them to 1.58bit, we can massively shrink the model!
*   We provided our dynamic quantization code as a fork to llama.cpp: [github.com/unslothai/llama.cpp](https://github.com/unslothai/llama.cpp)
*   We leveraged [Bartowski](https://huggingface.co/bartowski)â€™s importance matrix for the lower quants.

ğŸ’¬ Chat Template Issues

All distilled versions and the main 671B R1 model use the same chat template:`<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>What is 1+1?<ï½œAssistantï½œ>It's 2.<ï½œendâ–ofâ–sentenceï½œ><ï½œUserï½œ>Explain more!<ï½œAssistantï½œ>`A BOS is forcibly added, and an EOS separates each interaction. To counteract double BOS tokens during inference, you should only call tokenizer.encode(..., add\_special\_tokens = False) since the chat template auto adds a BOS token as well.  
For llama.cpp / GGUF inference, you should skip the BOS since itâ€™ll auto add it.`<ï½œUserï½œ>What is 1+1?<ï½œAssistantï½œ>`The <think\> and </think\> tokens get their own designated tokens. For the distilled versions for Qwen and Llama, some tokens are re-mapped, whilst Qwen for example did not have a BOS token, so <|object\_ref\_start|\> had to be used instead.Tokenizer ID Mappings:

| Token | R1 | Distill Qwen | Distill Llama |
| --- | --- | --- | --- |
| <think\> | 128798 | 151648 | 128013 |
| </think\> | 128799 | 151649 | 128014 |
| <|begin\_of\_sentence|\> | 0 | 151646 | 128000 |
| <|end\_of\_sentence|\> | 1 | 151643 | 128001 |
| <|User|\> | 128803 | 151644 | 128011 |
| <|Assistant|\> | 128804 | 151645 | 128012 |
| Padding token | 2 | 151654 | 128004 |

Original tokens in models:

| Token | Qwen 2.5 32B Base | Llama 3.3 70B Instruct |
| --- | --- | --- |
| <think\> | <|box\_start|\> | <|reserved\_special\_token\_5|\> |
| </think\> | <|box\_end|\> | <|reserved\_special\_token\_6|\> |
| <ï½œbeginâ–ofâ–sentenceï½œ\> | <|object\_ref\_start|\> | <|begin\_of\_text|\> |
| <ï½œendâ–ofâ–sentenceï½œ\> | <|endoftext|\> | <|end\_of\_text|\> |
| <ï½œUserï½œ\> | <|im\_start|\> | <|reserved\_special\_token\_3|\> |
| <ï½œAssistantï½œ\> | <|im\_end|\> | <|reserved\_special\_token\_4|\> |
| Padding token | <|vision\_pad|\> | <|finetune\_right\_pad\_id|\> |

All Distilled and the original R1 versions seem to have accidentally assigned the padding token to <ï½œendâ–ofâ–sentenceï½œ\>, which is mostly not a good idea, especially if you want to further finetune on top of these reasoning models. This will cause endless infinite generations, since most frameworks will mask the EOS token out as -100.We fixed all distilled and the original R1 versions with the correct padding token (Qwen uses <|vision\_pad|\>, Llama uses <|finetune\_right\_pad\_id|\>, and R1 uses <ï½œâ–padâ–ï½œ\> or our own added <ï½œPADâ–TOKENï½œ\>.

ğŸ–¥ï¸ Running Dynamic Quants

You do NOT need to use a new llama.cpp version - any system (like Ollama, [OpenWebUI](https://github.com/open-webui/open-webui), Transformers, even vLLM) which can run GGUFs should be able to run dynamic quants. It might be slow if you do not have enough VRAM or RAM, but it works.If you want to use llama.cpp directly, follow the build instructions for llama.cpp [here](https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md) - donâ€™t forget to enable GPU support! I normally use the below:

```
apt-get update
apt-get install build-essential cmake curl libcurl4-openssl-dev -y
git clone https://github.com/ggerganov/llama.cpp
cmake llama.cpp -B llama.cpp/build \
	-DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON
cmake --build llama.cpp/build --config Release -j --clean-first --target llama-quantize llama-cli
cp llama.cpp/build/bin/llama-* llama.cpp
```Then download the model through [huggingface.co/unsloth/DeepSeek-R1-GGUF](https://huggingface.co/unsloth/DeepSeek-R1-GGUF) You can use Hugging Face for this. To download the 1.58bit version, do:```
# pip install huggingface_hub
from huggingface_hub import snapshot_download
snapshot_download(
    repo_id = "unsloth/DeepSeek-R1-GGUF",
    local_dir = "DeepSeek-R1-GGUF",
    allow_patterns = [â€œ*UD-IQ1_S*â€],
)
```This will download 3 GGUF files to DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1\_S. Then, use this formula to decide how many layers you can offload to the GPU. If you do not have a GPU, set offloading to 0:n offload \= VRAM â¡ ( G â¢ B ) Filesize â¡ ( G â¢ B ) Ã— n layers âˆ’ 4

DeepSeek R1 has 61 layers. For example with a 24GB GPU or 80GB GPU, you can expect to offload after rounding down (reduce by 1 if it goes out of memory):

| Quant | File Size | 24GB GPU | 80GB GPU | 2x80GB GPU |
| --- | --- | --- | --- | --- |
| 1.58bit | 131GB | 7 | 33 | All layers 61 |
| 1.73bit | 158GB | 5 | 26 | 57 |
| 2.22bit | 183GB | 4 | 22 | 49 |
| 2.51bit | 212GB | 2 | 19 | 32 |

To run the model, we quantize the K cache to 4bit. Quantizing the V cache requires flash attention kernels to be compiled for llama.cpp. We use all threads on the machine, and use the recommended temperature by DeepSeek of 0.6. The context size is how many tokens you want the model to generate.```
./llama.cpp/llama-cli \
	--model DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \
	--cache-type-k q4_0 \
	--threads 12 -no-cnv --n-gpu-layers 61 --prio 2 \
	--temp 0.6 \
	--ctx-size 8192 \
	--seed 3407 \
	--prompt "<ï½œUserï½œ>Create a Flappy Bird game in Python.<ï½œAssistantï½œ>â€
```

ğŸ¦™ Run in Ollama/vLLM
---------------------

If you want to use [Ollama](https://github.com/ollama/ollama) or [vLLM](https://docs.vllm.ai/en/latest/features/quantization/gguf.html) for inference on GGUFs, you need to first merge the 3 GGUF split files into 1 like the code below. Then you will need to run the model locally.```
./llama.cpp/llama-gguf-split --merge \
DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \
	merged_file.gguf
```

ğŸ’¡ Prompt and results

The full prompt used is below:  
Create a Flappy Bird game in Python. You must include these things:

*   You must use pygame.
*   The background color should be randomly chosen and is a light shade. Start with a light blue color.
*   Pressing SPACE multiple times will accelerate the bird.
*   The bird's shape should be randomly chosen as a square, circle or triangle. The color should be randomly chosen as a dark color.
*   Place on the bottom some land colored as dark brown or yellow chosen randomly.
*   Make a score shown on the top right side. Increment if you pass pipes and don't hit them.
*   Make randomly spaced pipes with enough space. Color them randomly as dark green or light brown or a dark gray shade.
*   When you lose, show the best score. Make the text inside the screen. Pressing q or Esc will quit the game. Restarting is pressing SPACE again.

The final game should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section.Full tables and results at: [docs.unsloth.ai/basics/deepseek-r1-dynamic-1.58-bit](https://docs.unsloth.ai/basics/deepseek-r1-dynamic-1.58-bit)  
All 18 outputs and Python generated code are also uploaded there!

ğŸ’• Thank you!

As usual, a huge thank you to everyone for using & sharing Unsloth - we really appreciate it. ğŸ™As always, be sure to join our [Reddit page](https://www.reddit.com/r/unsloth/) and [Discord](https://discord.gg/unsloth) server for help or just to show your support! You can also follow us on [Twitter](https://twitter.com/unslothai) and [newsletter](https://unsloth.ai/newsletter).

Thank you for reading!

Daniel & Michael Han ğŸ¦¥  
27 Jan 2025

