---
title: 从 AI 编年史到继续发呆
date: 2025-03-23
extra:
  source: https://hiwannz.com/archives/1159.html
  original_title: 从 AI 编年史到继续发呆
---
## Summary
**摘要**：
**摘要**：
作者分享了自己近期工作中的思考，围绕着AI技术在实际业务中的应用展开讨论。文章从客户对AI产品集成的需求出发，客户希望产品可以快速集成AI能力，并提出了一些疑问，如对Deepseek的露出和差异化效果的期待，以及C2C模式在AI产品中的应用。随后，作者回顾了AI的发展历程，介绍了图灵、冯·诺依曼、罗森布拉特、闵斯基与佩帕特、费根鲍姆、珀尔、杰弗里·辛顿、杨立昆、本希奥、瓦普尼克、霍普菲尔德、施密德胡伯、古德费洛、达里奥等AI发展史上的关键人物及其贡献。这些人物在理论基础、算法创新、应用实践和伦理思考等方面推动了AI的进步。作者也提到了AI发展并非一帆风顺，并表达了对当前AI发展阶段的看法，认为已经度过了“通用智能”的探索阶段，下一步需要对准通用人工智能的方向进行细化和延伸。最后，作者结合自身使用AI的体验，分享了对AI的复杂情感，既受益于AI带来的效率提升，又对过度信任AI可能带来的问题表示担忧，并对AI改变世界的进度持观望态度。

**要点总结**：

1.  AI在实际业务中的应用：客户对AI产品集成的需求，以及在实际应用中遇到的问题，如产品差异化和商业化模式的探索。
    - 客户希望产品能够快速集成AI能力，但同时也对产品的实际效果和差异化提出了更高的要求。许多公司通过集成云市场的AI服务或借鉴国外AI应用来快速搭上AI快车，但真正的商业化道路仍在探索中。

2.  AI发展史上的关键人物及其贡献：回顾了AI发展历程中的重要人物，从图灵的图灵测试到Hinton的深度学习，再到古德费洛的GAN，这些科学家奠定了AI发展的基石。
    - 文章详细介绍了AI领域多位先驱的贡献，如艾伦·图灵奠定了AI的哲学基础和测试标准，冯·诺依曼提出了计算机的架构，辛顿推动了深度学习的复兴，古德费洛发明了GAN等。

3.  AI发展并非一帆风顺：AI的发展经历了几次“AI冬天”，如感知机的失败和专家系统的局限性，但每一次低谷都为AI的下一次飞跃积攒了力量。
    - 神经网络的研究曾因感知机的局限性而陷入低谷，但辛顿通过反向传播算法解决了这个问题，使得神经网络重新崛起。

4.  从“通用智能”到通用人工智能：AI的发展已经度过了“通用智能”的探索阶段，下一步可能要对准通用人工智能的方向进行进一步的细化和延伸。
    - 目前AI已经度过了“AI行不行”的疑惑，但“如何让他更安全，更有效”仍然是一个短期内无法解决的问题。

5.  对AI的复杂情感：既受益于AI带来的效率提升，又对过度信任AI可能带来的问题表示担忧，并对AI改变世界的进度持观望态度。
    - 作者在实际使用AI的过程中，体会到了AI带来的效率提升，但也对过度信任AI可能导致的代码bug等问题表示担忧。

## Full Content
Title: 从 AI 编年史到继续发呆 - 见字如面

URL Source: https://hiwannz.com/archives/1159.html

Markdown Content:
![Image 1](https://media.hiwannz.cn/wp-content/uploads/2025/03/1742701019-juan-rumimpunu-nLXOatvTaLo-unsplash-1024x683.jpg!/format/webp/lossless/true)

这是一篇来自近期工作发呆时的思考（说做是总结和记录可能更为恰当）。

从2024年的中下旬开始，公司就一直和我们铺垫说有一个客户在项目交付中出现了一些问题，由于项目合同的金额较大，客户在内部沟通的过程中要求我们将一部分项目的回款使用线下交付的方式进行验收，说白了就是有一部分同事需要到客户现场去驻场。于是我们就在年后冲向北京，这篇文章也是我在工作业余时的胡思乱想。

熟悉我的朋友应该都知道，我在工作中时不时要参与到一些诸如客户支持，定价沟通，产品价值talking 的环节中，可能是这两年大家都把 AI 作为了“年度话题”的重要性，所以总会有一些客户想要进一步了解“产品如何在实际的业务流中快速集成 AI 的能力”，市面上也有各种各样吹嘘“自己的产品又一次集成了 AI”的 PR 文章，但本质上其实大都是在云市场集成 AI 之后快速实现了一个 chatbot，好像效果并没有那么好。

_当然也有一些客户会来问一些在不同视角的问题，我听过的问题印象比较深的就是“产品集成了 AI 能力我是认可的，但是这个产品中我看不到 Deepseek 的露出，你们怎么处理”，“在产品中集成 LLM 其实各家都大差不差，但是差异性的效果我暂时还没有看到”，此外在一些类似的产品中我发现 C2C（Copy to china） 的思路目前可能还是奏效的，去 ProductHunt 或者类似的网站看看国外的“同行们”又搞出来了哪些 AI 相关的应用，然后看看哪一个最适合集成到自己的项目中，砍掉一些复杂功能再做一些本地化，好像给自己的产品也就搭上了 AI 这趟快车。_

有一些产品会说到自己在业务中使用 MCP（Model Context Protocol）和 RAG（Retrieval-Augmented Generation）来提供更加全面的大模型能力支持，从逻辑上来说在产品代码中能够真的提升效率和准确度，基于一些比如 Dify 或者 FastGPT 的产品做二次开发好像也能做到进一步的实践与尝试（没错，我们的产品也提供了这样的能力），但从最终愿意买单并且用于真实企业内部业务流程的状态来看，我觉得大家更多是想摸着石头过河再观望看看有哪些商业化的思路。

昨天和同事聊天的时候说到不同行业中的门槛其实还比较高，可能互联网行业的从业者大都掌握了无痛访问 Google 或者 Github 等网站的方式，但其实还有非常多的老百姓不太分得清其中的区别（事实上互联网从业者也不见得都掌握了这个能力），对于老百姓来说耳熟能详的张一鸣和王兴兴是那种“在某一个行业中实现了成功的例子”，但是对他们到底在做什么其实并不清楚，其实说到 AI，说到人工智能，这应该是一个伴随计算机有 N 多年历史的故事了。

但是 AI 到底是咋来的？好像前些年我们对 AI 的理解和认知还停留在 TensorFlow 和 Pytorch 这样的算法中，怎么一眨眼 AI 就已经飞入寻常百姓家了？

既然聊到了这里，我就来试试讲讲 AI 发展的一系列关键人物**（万一说错，还请拍砖）**。

### 图灵，计算机能否像人一样思考？

![Image 2](https://media.hiwannz.cn/wp-content/uploads/2025/03/1742708219-Xnip2025-03-23_13-36-32-1024x562.jpg!/format/webp/lossless/true)

1940 年，二战如火如荼，德国的“恩尼格玛（Enigma）”密码机几乎让所有盟军的情报系统陷入瘫痪。英军情报部门召集了一群数学家，他们的任务是——破解 Enigma，让德国的情报不再是个谜。

这群数学家中，有一个瘦高、害羞但聪明绝顶的年轻人，他叫艾伦·图灵。

他不是普通的数学家，他构想了一种“通用计算机”——一种可以执行任何计算任务的机器，并用它来破解 Enigma。他发明了“炸弹机（Bombe）”，最终成功解码了德军密码，让二战提前结束了两年。

然而，他并不满足于此。他问了一个更大的问题：

“如果机器能够进行计算，是否意味着它也能思考？”

他提出了著名的“图灵测试”——如果一个人无法区分是在与人还是与机器对话，那么机器就具备了“智能”。这个想法为现代人工智能奠定了基础。

_大多数人最快捷大概了解图灵的方式就是那一部由本尼迪克特·康伯巴奇主演的“模仿游戏”，在二战期间图灵在英国政府的雇佣下破解了德军的“恩尼格码”密码机，由此也奠定了现代计算机科学的基础。在他 16 岁的时候就开始阅读爱因斯坦的相关著作，在他 19 岁的时候就考入了剑桥大学开始攻读数学本科，并且在22 岁时候以优异的成绩毕业。_

_虽然图灵是一名数学家，在学习数理逻辑学（就是我们学的那个“与”，“非”，“或”等等的学科）的时候又开始对逻辑学，哲学进行了更加深入的研究。但虽然图灵奠定了人工智能的哲学基础，也提出了计算理论与 AI 的测试标准，但由于同性恋的原因受到迫害，在 41 岁的时候英年早逝。_

### 冯诺依曼，计算机如何高效存储和计算？

![Image 3](https://media.hiwannz.cn/wp-content/uploads/2025/03/1742708599-Xnip2025-03-23_13-42-36-1024x562.jpg!/format/webp/lossless/true)

如果说图灵是计算机科学的哲学家，那么冯·诺依曼（John von Neumann）就是计算机的工程师。

在 1945 年，他提出了一种全新的计算机架构：把数据和程序存储在同一个内存里，让计算机可以自动执行指令。这就是后来所有计算机都遵循的“冯·诺依曼架构”，它让计算机变得真正实用。

除了计算机，他还发明了博弈论，并且是最早研究人工智能如何决策的人之一。

_我相信每一个计算机相关专业的同学应该都听过冯诺依曼，比如在计算机原理的课程上肯定会学到他提出的冯诺依曼架构。此外他也提出了能让程序指令和数据能够存储在同一个存储器中的存储程序概念，从而让计算机可以自动执行程序。_

_值得一提的是冯·诺伊曼从小就以过人的智力与记忆力而闻名。他在一生中发表了大约150篇论文，其中有60篇纯数学论文，20篇物理学以及60篇应用数学论文。他最后的作品是一个在医院未完成的手稿，后来以书名《计算机与人脑》（The Computer and the Brain）发布，表现了他生命最后时光的兴趣方向（但其实冯诺依曼不仅在计算机方向有建树，他也是博弈论之父）。_

### 罗森布拉特，能否让计算机自己学习？

![Image 4](https://media.hiwannz.cn/wp-content/uploads/2025/03/1742709765-Xnip2025-03-23_13-59-16-1024x562.jpg!/format/webp/lossless/true)

1958 年，弗兰克·罗森布拉特提出了一个让整个 AI 领域兴奋的想法——“感知机（Perceptron）”，它是一种最简单的神经网络，可以通过调整权重来学习模式，比如识别简单的形状。“创造具有人类特质的机器，一直是科幻小说里一个令人着迷的领域。但我们即将在现实中见证这种机器的诞生，这种机器不依赖人类的训练和控制，就能感知、识别和辨认出周边环境。”

然而，1969 年，闵斯基（Marvin Minsky）和派普特（Seymour Papert） 在《感知机（Perceptrons）》一书中证明，感知机无法解决像“异或”这样的基本问题，这让整个 AI 研究陷入了“AI 冬天”，神经网络被主流科学界抛弃。这本书抨击了罗森布拉特的工作，并本质上终结了感知机的命运。

罗森布拉特没能渡过AI的寒冬。1971年，他在43岁生日那天，在切萨皮克湾（Chesapeake Bay）乘单桅帆船出海时溺水身亡。

_理论上来说，感知机其实是第一个尝试让机器“学习”的模型，但它的失败让神经网络沉寂了 20 年，直到 x辛顿重新挖掘它。_

### 闵斯基与佩帕特，感知机的局限性是什么？

![Image 5](https://media.hiwannz.cn/wp-content/uploads/2025/03/1742711638-image-1024x562.png!/format/webp/lossless/true)

![Image 6](https://media.hiwannz.cn/wp-content/uploads/2025/03/1742712067-Xnip2025-03-23_14-40-49-1024x562.jpg!/format/webp/lossless/true)

1956 年，达特茅斯会议 上，一群科学家聚在一起，试图定义“人工智能” 这个领域。其中，闵斯基作为 MIT 人工智能实验室的创建者，是符号主义 AI 的坚定支持者。

他的梦想很宏大：“AI 应该像人一样思考，我们只要给它足够的逻辑规则，它就能成为真正的智能。” 他的研究主要基于符号逻辑，比如他开发了一种叫做 Lisp 机器 的计算机，专门用来运行 AI 代码。

与此同时，佩珀特则更加关注机器学习和儿童教育，他认为计算机应该像孩子一样学习，而不是依赖固有规则。他发明了一种编程语言——Logo，可以让孩子通过简单的指令控制“小乌龟”在屏幕上画图形。他们二位的 AI 研究，让 AI 在 1960 年代成为了学术界的明星，政府和企业纷纷投资，AI 似乎要迎来一个黄金时代！

但好景不长，感知机（Perceptron） 的失败让闵斯基和佩珀特觉得，神经网络完全没戏。他们在 1969 年合著了一本书——《Perceptrons》，直接指出了感知机的致命缺陷“感知机无法解决“异或（XOR）”问题——也就是说，它没办法学会“如果 A 和 B 相同，输出 0，否则输出 1” 这样的简单逻辑。”

他们的批评毁灭性地打击了神经网络研究，导致 1970 年代 AI 研究资金骤减，进入了第一次“AI 冬天”。

_虽然闵斯基和佩珀特让神经网络陷入低谷，但他们的研究也推动了 AI 其他方向的发展。_

_闵斯基继续研究“心智架构”，提出了“框架理论”（Frame Theory）——AI 应该拥有类似人类的知识结构，而不是单纯的数据处理器。佩珀特专注于教育领域，创造了建构主义学习理论，他的 Logo 语言影响了后来的 Scratch 和 Python 在教育领域的应用。_

_直到 1980 年代，辛顿通过反向传播算法解决了感知机的问题，才让神经网络重新崛起。但讽刺的是，闵斯基并不认同深度学习，他仍然认为符号 AI 才是未来。_

### 费根鲍姆，AI 能否模仿人类专家？

![Image 7](https://media.hiwannz.cn/wp-content/uploads/2025/03/1742712383-Xnip2025-03-23_14-46-04-1024x562.jpg!/format/webp/lossless/true)

在 1960-1970 年代，人工智能的主流研究方向是通用智能（General AI），也就是让机器能像人一样思考。但费根鲍姆另辟蹊径，他提出了一个完全不同的想法：

“我们不需要让 AI 变得像人一样聪明，我们只需要让 AI 变得像‘某个领域的专家’一样聪明。”

他认为，与其让 AI 学会所有事情，不如让它深耕某一个领域，积累大量的专业知识，成为一个真正的“专家”。这就是“专家系统（Expert System）”的概念——基于规则、逻辑推理和专业知识，让 AI 在特定领域内表现出专家级的能力。

_费根鲍姆的第一个专家系统项目是1965 年的DENDRAL，它是一个帮助化学家分析分子结构的 AI。紧接着在1970 年又推出了MYCIN——一个医疗诊断专家系统。虽然由于当时的法律和伦理问题使得医生不敢完全相信机器的诊断，这个产品也没真的用在医院中，但它的成功证明了 AI 可以在专业领域中成为真正的专家。_

###   
**珀尔**，AI 如何进行不确定性推理？

![Image 8](https://media.hiwannz.cn/wp-content/uploads/2025/03/1742712841-image-1024x425.png!/format/webp/lossless/true)

在 20 世纪 80 年代，人工智能主要依赖概率统计和模式识别，但它无法理解因果关系。朱迪亚·珀尔认为，真正的智能必须知道“为什么”——比如，吸烟和肺癌有关，但到底是因果关系，还是仅仅相关？

他提出了贝叶斯网络，用数学方式描述变量之间的因果联系，让 AI 具备更强的推理能力。后来，他又发展出因果推理和反事实思维，让 AI 不仅能预测，还能回答“如果情况不同，结果会怎样？”。这些理论如今影响着数据科学、医疗 AI、经济学，甚至推动下一代更智能的 AI 发展。

_珀尔的因果推理思想，彻底改变了 AI 的研究方向。过去，AI 主要依赖深度学习，但神经网络的一个问题是它们只会发现模式，而不会理解因果。_

_比如传统 AI 可能发现：夏天卖冰淇淋的同时，游泳馆的溺水率也会上升。但因果 AI 知道：冰淇淋不会导致溺水，真正的原因是夏天气温升高。他的著作《为什么（The Book of Why）》深入探讨了因果推理的重要性，这为现代 AI 的解释能力奠定了基础。_

### 杰弗里辛顿，如何训练深度神经网络？

![Image 9](https://media.hiwannz.cn/wp-content/uploads/2025/03/1742710583-Xnip2025-03-23_14-08-34-1024x562.jpg!/format/webp/lossless/true)

1970 年代，神经网络研究遭遇寒冬。当时的主流 AI 研究者（如闵斯基和佩珀特）认为神经网络太简单，无法解决复杂问题。许多科学家纷纷放弃，但辛顿偏偏选择了这条“错误的道路”。

辛顿出生于英国，外祖父是著名数学家 George Boole（布尔代数的创始人），他从小就喜欢挑战权威。在攻读博士期间，他研究反向传播算法（Backpropagation），一种可以让神经网络自动调整权重的方法。尽管这个算法早已在 1970 年被提出，但几乎没人相信它真的能让 AI 学习。Hinton 和他的团队坚持优化反向传播，并在 1986 年成功证明它可以让多层神经网络高效学习复杂任务。

90 年代，辛顿继续探索更深层的神经网络，并提出受限玻尔兹曼机（RBM） 和 深度信念网络（DBN），成为“深度学习”（Deep Learning）概念的奠基人之一。到了 2012 年，他的学生 Alex Krizhevsky 使用卷积神经网络（CNN） 赢得 ImageNet 竞赛，标志着深度学习时代的正式到来。

后来，辛顿还提出了 Transformer 的早期雏形——胶囊网络（Capsule Network），并成为 Google Brain 的重要研究员，推动 AI 革命。他的坚持让神经网络从 20 世纪的冷门理论，变成了今天席卷全球的 AI 基石。

_1980 年代，辛顿和他的团队证明了一个重要理论——“反向传播（Backpropagation）”，可以让神经网络通过调整权重进行学习。但当时的计算机性能不够强大，神经网络仍然没能流行起来。_

_时间来到 2012 年，Hinton 的学生 Alex Krizhevsky 训练了一种深度卷积神经网络（AlexNet），在 ImageNet 竞赛上击败了所有传统算法。这标志着深度学习的崛起，AI 从此进入了一个全新的时代！_

### 杨立昆，计算机如何识别图像？

![Image 10](https://media.hiwannz.cn/wp-content/uploads/2025/03/1742713685-image-1024x562.png!/format/webp/lossless/true)

在 20 世纪 80 年代，计算机视觉仍然是个难题。杨立昆认为，人工智能不应依赖手工设计的规则，而应该“像人一样”通过学习数据自动提取特征。他结合反向传播算法和神经网络，发明了卷积神经网络（CNN），让计算机能自动识别图像中的模式。

90 年代，他的 CNN 被用于手写数字识别，并成为美国银行支票识别系统的一部分。但深度学习当时还不够流行，他的研究一度被冷落。直到 2010 年后，计算能力的提升让 CNN 迎来爆发，成为计算机视觉的核心技术，被广泛应用于人脸识别、自动驾驶和医疗影像分析。

如今，杨立昆继续推动 AI 向自监督学习发展，试图让 AI 更接近人类的大脑学习方式，而不仅仅依赖海量数据进行训练。

_杨立昆的原来中文译名为：扬·勒丘恩，2017年他在中国的演讲提供了正式的中文姓名。他法文的姓是（Le Cun），到美国之后，很多人都误认为Le是中间名，所以他在20世纪八九十年代把自己的姓的拼法改成了LeCun。_

### 本希奥，AI需要遵循伦理吗？

![Image 11](https://media.hiwannz.cn/wp-content/uploads/2025/03/1742715609-image-1024x556.png!/format/webp/lossless/true)

本希奥与辛顿和杨立昆并称为“深度学习三巨头”。他是神经网络研究的先驱之一，推动了深度学习的数学基础，并对无监督学习、序列建模、注意力机制（Transformer 的前身）等领域作出了重大贡献。

Bengio 1964 年出生于法国的一个知识分子家庭，后来随家人移民到加拿大。他在蒙特利尔大学攻读计算机科学博士学位，师从 AI 研究者 René D. Mori，并开始专注于神经网络的学习方法。当时，神经网络在学术界并不被看好，但本希奥坚信它们能够超越传统的统计机器学习方法。

在 2000 年代，本希奥率先研究如何让神经网络自动学习数据的抽象特征，并提出了逐层训练（layer-wise pretraining）的方法，使得更深层的网络能够高效训练。这为后来的卷积神经网络（CNN）和递归神经网络（RNN）奠定了数学基础。他的研究极大地推动了深度学习的复兴，影响了 ImageNet 竞赛的突破（2012），并为后来的 Transformer 架构铺平了道路。

2014 年，本希奥的团队提出了注意力机制（Attention Mechanism），这是一种让神经网络自动关注最重要信息的技术。这项技术很快被 Google 研究员 Vaswani 等人发展为 Transformer 架构，并成为GPT-4、BERT、Claude 以及几乎所有现代 LLM 的基础。

可以说，本希奥间接塑造了现代大模型，他的研究影响了 AI 在自然语言处理、计算机视觉等领域的所有突破性进展。

_与辛顿和杨立昆不同，本希奥在 AI 伦理和社会责任方面表现得更加谨慎。当 ChatGPT 这样的 LLM 开始爆发时，他曾公开警告AI 可能会对社会产生巨大影响，呼吁制定更严格的 AI 监管和伦理框架。_

_2018 年，他与辛顿、杨立昆共同获得了图灵奖（计算机领域的最高荣誉），正式确立了他在 AI 领域的历史地位。_

_尽管他是深度学习最重要的奠基人之一，但他并没有像 OpenAI 或 DeepMind 那样主导商业化 AI 公司的发展。他的研究主要在学术界，而他的许多学生（如 Transformer 论文作者 Vaswani）却推动了 AI 工业化的浪潮。_

### 瓦普尼克，如何找到最优分类方式？

![Image 12](https://media.hiwannz.cn/wp-content/uploads/2025/03/1742714007-image-1024x562.png!/format/webp/lossless/true)

在 20 世纪 60 年代的苏联，数学家瓦普尼克和他的导师 Alexey Chervonenkis 共同研究如何让机器像人一样学习。他们意识到，AI 不能只死记硬背训练数据，而应该学会“泛化”——即用有限的经验推断新的知识。这促使他们提出统计学习理论（Statistical Learning Theory, SLT），并发明了支持向量机（SVM）。

SVM 的核心思想是找到数据之间的最优分界线，使得新数据也能被正确分类。这个方法在 90 年代被西方计算机科学界发现，并迅速成为机器学习的主流算法之一，在文字识别、生物信息学、金融分析等领域大放异彩。

尽管 SVM 一度是机器学习的黄金标准，但瓦普尼克对深度学习持保留态度，认为它依赖海量数据而缺乏理论上的优雅。他的理论为现代 AI 奠定了数学基础，使机器学习不再是经验主义，而成为一门严谨的科学。

_瓦普尼克是一位纯数学派的科学家，支持向量机（SVM）在 1990s 成为了机器学习领域的标准方法。在深度学习出现之前，SVM 在很多任务上都被认为是最强的学习算法之一。_

### 霍普菲尔德，神经网络如何进行联想记忆？

![Image 13](https://media.hiwannz.cn/wp-content/uploads/2025/03/1742714544-image-1024x416.png!/format/webp/lossless/true)

在 20 世纪 80 年代的人工智能研究领域，神经网络几乎被主流学术界遗忘。许多研究者转向了符号主义 AI（Symbolic AI）或专家系统（Expert Systems），但霍普菲尔德这个本职是物理学家的科学家却意外地为神经网络带来了一次重要的复兴。

霍普菲尔德早年是一位研究凝聚态物理的学者，他的兴趣集中在复杂系统如何自组织。在 1982 年，他提出了一种全新的能量模型，即霍普菲尔德神经网络，这是一种受物理学自洽场理论启发的神经网络模型。他证明了这个网络可以用来进行联想记忆（Associative Memory），即只需要输入部分信息，网络就能恢复出完整的模式。这种方法不同于传统的符号 AI，而是模拟了大脑神经元的工作方式。

霍普菲尔德神经网络的提出激发了 AI 研究者对神经网络的兴趣，为 1980 年代后期的神经网络复兴铺平了道路。辛顿和杨立昆等后来的 AI 研究者也深受他的影响。

尽管霍普菲尔德主要贡献在物理学领域，他的跨界工作却成为神经网络历史上的关键节点，让 AI 研究重新回到了仿生学的道路上。

_霍普菲尔德神经网络在数学上证明了这个网络一定能够收敛，从而对基于神经网络的人工智能产生了奠基性的影响，开启了连接主义深度学习的大门。_

### 施密德胡伯，如何让神经网络记住长期信息？

![Image 14](https://media.hiwannz.cn/wp-content/uploads/2025/03/1742715004-image-1024x416.png!/format/webp/lossless/true)

施密德胡伯是深度学习领域最重要的奠基者之一，他的研究直接影响了现代 AI，尤其是在自然语言处理和序列数据建模中的应用。他最著名的贡献之一就是LSTM（长短时记忆网络），这项技术后来成为谷歌、苹果、OpenAI 以及众多企业训练神经网络的核心方法。

施密德胡伯生于 1963 年，从小就展现出极高的数学天赋。他在瑞士学习计算机科学和人工智能，很早就对人工智能的终极目标产生了浓厚兴趣——创造一个能够自主学习、不断进化的人工智能。

在 20 世纪 90 年代，神经网络在处理长序列数据（如文本、语音和时间序列数据）时遇到了“梯度消失”问题：传统的循环神经网络（RNN）无法记住过长时间跨度的信息。

1997 年，施密德胡伯和他的学生 Sepp Hochreiter 共同发明了长短时记忆网络（Long Short-Term Memory，LSTM），这种架构通过引入“门控机制”来有效存储和传递信息，解决了梯度消失的问题。这项发明在当时并没有被广泛认可，但在 2010 年代，随着计算能力的提升和大规模数据训练的普及，LSTM 迅速成为语音识别、机器翻译、文本生成等领域的主流技术。

施密德胡伯的野心远远不止于 LSTM，他一直强调创造真正的通用人工智能（AGI）。他认为 AI 研究应该专注于元学习（meta-learning），即让 AI 学会如何自主学习，并不断优化自身。

他提出了“人工科学家”（Artificial Scientist）这一概念，认为 AI 未来应该能够自主提出假设、设计实验，并发现新的知识，就像真正的科学家一样。

_尽管施密德胡伯的贡献不可否认，他的知名度远远低于辛顿、杨立昆和本希奥，部分原因是 LSTM 的商业应用直到 2010 年代才开始爆发。此外，他曾多次公开表达对 DeepMind 和 OpenAI 的不满，认为这些机构“没有给予他的研究足够的认可”。_

_尽管如此，_施密德胡伯_仍然是现代 AI 领域不可忽视的奠基者。今天的 GPT-4、Suno 音乐 AI、DeepMind 的 AlphaFold 等许多应用都间接或直接受益于他的研究，他的 LSTM 仍然在许多 AI 系统中发挥作用。_

_在 AI 发展的历史中，_施密德胡伯_是一个极具远见的人，他不仅改变了深度学习的技术基础，也为未来的 AGI 研究提供了重要的方向。_

### 古德费洛，AI 能否创造新内容？

![Image 15](https://media.hiwannz.cn/wp-content/uploads/2025/03/1742716072-image-1024x555.png!/format/webp/lossless/true)

古德费洛 1985 年出生于美国怀俄明州，他从小展现出非凡的数学和编程才能。大学时期，他就读于斯坦福大学，主修计算机科学。随后，他进入加拿大蒙特利尔大学，师从深度学习三巨头之一的本希奥，正式踏入神经网络研究领域。

在本希奥的实验室里，他接触到了深度学习和生成模型，并开始探索如何让 AI 生成逼真的图像。这一探索最终促成了GAN（生成对抗网络）的诞生。

2014 年，古德费洛在博士研究期间的一次讨论中，和同事争论如何让神经网络自主生成更真实的图像。当时的 AI 生成模型（如变分自编码器 VAE）仍然很难生成高清且自然的图片。

突然，他灵光一闪，想出了一个革命性的概念：让两个神经网络互相竞争！他的想法是：一个 AI（生成器，Generator） 负责生成假图像。另一个 AI（判别器，Discriminator） 负责判断这些图像是真实的还是伪造的。二者不断博弈，最终生成器能骗过判别器，生成高度逼真的图像！

这种“对抗学习”的方式，突破了传统 AI 生成方法的局限，被命名为 GAN（Generative Adversarial Network）。

_GAN 让 AI 从“分析数据”变成了“创造数据”，彻底改变了 AI 在艺术、设计、游戏、影视等行业的应用方式。可以说，他的研究让 AI 从理解世界进化到了创造世界，并成为 AI 生成内容（AIGC）浪潮的奠基者之一。_

_古德费洛不仅是GAN 之父，也是AI 伦理的重要倡导者，他的贡献将长期影响 AI 发展方向。_

### 达里奥，AI 能否像人类一样写作和推理？

![Image 16](https://media.hiwannz.cn/wp-content/uploads/2025/03/1742716526-image-1024x586.png!/format/webp/lossless/true)

达里奥是 AI 研究领域的重要人物之一，曾在 OpenAI 领导多个关键项目，后创办 Anthropic，专注于 AI 安全与“AI 对齐”研究。他的工作推动了 AI 模型能力的飞跃，同时也让 AI 伦理问题进入公众视野。

达里奥最初是一名神经科学家，研究大脑与神经网络的相似性。他后来转向机器学习，加入 OpenAI，成为 AI 研究的核心人物之一。他在 OpenAI 期间的关键贡献包括：GPT-2 与 GPT-3 研究负责人：推动了现代大语言模型（LLM）的发展。AI 对齐研究的先驱：他提出 AI 需要“对齐人类价值观”，否则可能失控。

2021 年，达里奥离开 OpenAI，与几位前同事共同创立 Anthropic，专注于 AI 安全和“可控 AI”研究。Anthropic 的核心产品 Claude 系列（类似 ChatGPT）强调安全性，避免 AI 生成危险内容。他的研究强调：“AI 必须对人类有益，否则超级智能可能带来无法预测的后果。”

Anthropic 目前是 OpenAI 的主要竞争对手之一，并获得了 Google 近 30 亿美元的投资。

_当然，_说到达里奥我们其实也需要提到 Tom B Brown 和 Alec Radford，他们一行三个人的研究共同塑造了现代 AI 发展路径。但我想他们从 OpenA I跳槽到 Anthropic 也许还是遇到了那个难以抉择的问题“是追求更强大的 AI，还是追求更安全的 AI”？

### 算是本文的尾巴

写到这里其实我有点累了，事实上人工智能发展过程中总不是一帆风顺的，有兴趣的朋友可以看看维基百科上的“[人工智能史](https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%8F%B2)”，我相信一定会觉得收获满满，在这些厉害的科学家中也会存在各种奇怪的冲突（也正常，大家毕竟都是人嘛）。

但如果我们回到 2025 年的当下，会发现 AI 的发展已经度过了“通用智能”的探索阶段，下一步可能还是要对准通用人工智能的方向进行进一步的细化和延伸。由于各类基于 Claude 3.7 的产品我们已经基本跳过了“AI 行不行”的疑惑，但到底“如何让他更安全，更有效”还是一个短期内我们看不到答案的问题。

前一段时间木遥的解读“[vibe coding](https://mp.weixin.qq.com/s/yFR9ef1oYDRxj_oVviZw0A)”在朋友圈和各种渠道刷屏，文章中那句**“一方面它犹如神助，让你有一种第一次挥舞魔杖的幻觉。另一方面它写了新的忘了旧的，不断重构又原地打转，好像永远在解决问题但永远创造出更多新的问题，并且面对 bug 采取一种振振有词地姿态对你 gaslighting。你面对着层出不穷的工具甚至不知道自己该认真考虑哪个，心知肚明可能下个月就又有了新的「最佳实践」，养成任何肌肉记忆都是一种浪费，而所谓新的最佳实践只不过是用更快的速度产出更隐蔽的 bug 而已。**”可能也是许多正在与 AI 结对编程朋友的真实感觉。

但我想，AI 带来的改变确实日新月异，我能看到身边的朋友能够逐渐完成**“不相信 AI → 怀疑 AI → 全部用 AI → 不敢信任 AI → 再一次信任 AI”**的无限循环之中。我在一些业余时间也尝试练手用 AI 帮我写了几个产品，相比原先的产品设计与研发过程中，会发现现在的 AI 可能每一次都会比前一段时间的使用更加流畅一些，但依然无法完全避免上下文遇到限制导致记忆力幻觉或者相关的问题，这种感觉好像就像是一种慢性毒药，一方面更爽了，另一方面又不是那么爽。在产品设计过程中各种刷屏的什么“用 AI 搞定原型图，搞定高保真效果图”的论据其实也能变相让我们感知到 AI 在具体业务中的应用其实还处在比较早期的阶段。

一方面我受益于使用 AI 能够极大程度加快我把脑海中的某些想法付诸于实践的过程，但另一方面好像也能明显感知到过拟合带来的某种不适感，在开启新项目的时候确实能够通过 AI 极大程度加快效率，但是否会因为过度信任 AI 而导致代码中潜藏了许多暂时没有精力与时间发现的 bug，又变成代码中一个潜藏的问题真的很难一两句话讲清楚。

如果对比我前端时间那篇《[AI 取代人工进展走到哪一步了？](https://hiwannz.com/archives/1126.html)》，当下的我结论还是那句“保持对前沿技术学习与了解，让自己不要落伍的概念是没问题的，用 AI 来输出一下自己无处安放的创造力或者做一些创新与变化的真实落地是很好的”，但 AI 改变世界的进度条到哪一步了？

我觉得还得再看看。

