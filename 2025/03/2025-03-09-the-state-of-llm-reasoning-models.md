# The State of LLM Reasoning Models
- URL: https://sebastianraschka.com/blog/2025/state-of-llm-reasoning-and-inference-scaling.html
- Added At: 2025-03-09 01:34:30
- [Link To Text](2025-03-09-the-state-of-llm-reasoning-models_raw.md)

## Summary
**摘要**：
本文探讨了2025年大型语言模型（LLM）推理能力提升的研究进展，重点关注自DeepSeek R1发布以来涌现的推理时计算扩展方法。推理能力对于LLM解决复杂问题至关重要。提升LLM推理能力主要有两种策略：增加训练时计算量和增加推理时计算量。文章将LLM推理模型的改进方法分为四大类，并着重介绍了推理时计算扩展这一类别。推理时计算扩展是指在不修改模型权重的情况下，通过增加计算资源来提高模型推理能力，例如链式思考提示、投票和搜索策略。文章详细讨论了“s1: 简单测试时扩展”方法，该方法通过引入“等待”令牌来控制输出长度，并与其他推理扩展技术进行了比较。此外，文章还总结了其他11篇关于推理时计算扩展的研究论文，涉及测试时偏好优化、抑制过度思考、提高对抗鲁棒性、链式联想思考、自我回溯、潜在推理、计算最优扩展、推理时间计算基准、内部思考Transformer以及代码生成测试时扩展等多个方面。文章最后总结到，推理时计算扩展已成为提高LLM推理能力的热门研究方向，通过增加推理时的计算量，即使是较小的模型也能在推理基准测试中获得显著提升，缩小与大型模型之间的性能差距。

**要点总结**：

1.  **推理时计算扩展是提升LLM推理能力的重要方法**：通过在推理过程中增加计算资源，例如使用链式思考（CoT）提示，引导模型生成中间推理步骤，或者采用多数投票和波束搜索等策略，可以显著提高LLM在复杂任务中的准确性。
2.  **“s1: 简单测试时扩展”方法**：该方法通过在模型中引入“等待”令牌，类似于“逐步思考”的提示，从而控制模型的输出长度，实现对推理行为的有效控制，该方法属于有监督微调（SFT），通过预算强制，可以顺序扩展推理。
3.  **Test-Time Preference Optimization (TPO)通过迭代过程对齐LLM输出与人类偏好**：通过迭代的文本反馈来优化LLM的输出，使其更符合人类的偏好，在不改变模型权重的情况下，该方法通过多轮迭代优化模型的初始响应，以提升LLM的推理能力。
4.  **“Thoughts Are All Over the Place”研究关注LLM的“欠思考”现象**：该研究发现，推理模型在推理过程中频繁切换路径，导致问题解决准确率下降，因此提出了“思维切换惩罚”（TIP）方法，通过修改token的logits来抑制过早的推理路径切换，从而提高模型的推理能力。
5.  **Inference-Time Computations for LLM Reasoning and Planning一文对推理和规划任务中的各种推理时计算扩展技术进行了基准测试**：该研究评估了包括链式思考、树状思考和规划式推理在内的多种技术，在算术、逻辑、常识、算法推理和规划等11项任务中进行了测试，结果表明，虽然扩展推理时间的计算可以提高推理能力，但没有一种技术能够在所有任务中始终优于其他技术。
