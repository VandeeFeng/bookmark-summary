# Actual LLM agents are coming
- URL: https://vintagedata.org/blog/posts/designing-llm-agents
- Added At: 2025-03-15 07:41:50
- [Link To Text](2025-03-15-actual-llm-agents-are-coming_raw.md)

## Summary
**摘要**：
文章探讨了LLM（大型语言模型）驱动的智能代理的新发展，尤其关注OpenAI的DeepResearch和Claude Sonnet 3.7等模型在复杂任务中的应用。文章对比了传统的“工作流”式LLM应用与真正的LLM代理，强调了后者在规划、记忆和长期行动方面的优势。作者指出，简单地通过预定义提示和规则来使用LLM会陷入Richard Sutton提出的“苦涩的教训”，即短期有效的方法在长期会阻碍发展。真正的LLM代理需要通过强化学习进行训练，使其能够在复杂的环境中进行搜索、规划和行动。文章还提到了训练LLM代理的关键要素，包括强化学习、草稿生成、结构化数据以及多步骤训练。文章进一步探讨了如何通过模拟和仿真来扩展LLM代理的训练数据，并提出了一个可能的训练方案，包括创建网络搜索的模拟环境、预热模型、准备带有验证器的复杂查询、多步骤强化学习以及专注于最终综合的额外训练。最后，文章展望了LLM代理在搜索、网络工程和金融等领域的应用前景，并强调了民主化LLM代理的训练和部署的重要性。

**要点总结**：
1.  LLM代理的核心优势在于其能够动态地指导自身流程和工具使用，从而在完成任务时保持控制。与传统的工作流系统不同，真正的LLM代理能够进行规划，并在长期任务中有效行动，避免了简单预定义规则的局限性。
2.  强化学习（RL）是训练LLM代理的关键技术，通过奖励机制引导模型在复杂的环境中进行搜索和学习。验证器在RL训练中扮演重要角色，用于评估模型生成的输出是否符合预期目标，从而调整模型的行为。
3.  LLM代理的训练依赖于大量的“草稿”生成和评估，模型通过不断生成和评估文本来优化其逻辑序列和推理能力。这种训练方式允许模型在没有预定义提示的情况下，通过纯粹的推理来找到解决方案。
4.  为了提高训练效率和便于奖励验证，LLM代理的草稿通常被预定义为结构化数据，这有助于简化奖励验证过程，并在一定程度上促进整体推理过程。这种结构化方法可以被视为一种“rubric engineering”，通过预先定义的规则来引导模型的行为。
5.  文章提出了通过创建模拟环境来生成训练数据的方法，这对于那些缺乏足够真实数据的领域尤为重要。通过模拟，模型可以在一个虚拟的网络环境中进行搜索和学习，从而克服数据瓶颈，并提高其在实际应用中的性能。
