# Hallucinations in code are the least dangerous form of LLM mistakes
- URL: https://simonwillison.net/2025/Mar/2/hallucinations-in-code/
- Added At: 2025-03-03 14:52:14
- [Link To Text](2025-03-03-hallucinations-in-code-are-the-least-dangerous-form-of-llm-mistakes_raw.md)

## Summary
**摘要**：
文章作者指出，开发者在使用LLM生成代码时遇到的“幻觉”问题，即LLM创造不存在的方法或库，实际上是所有LLM错误中危害最小的。真正的风险在于LLM产生的代码中那些不会被编译器或解释器立即发现的错误。当运行LLM生成的代码时，任何虚构的方法都会立即报错，可以自行修复或反馈给LLM进行修正。与散文中的幻觉相比，代码具有强大的事实核查机制：运行代码，验证其是否有效，甚至可以利用ChatGPT Code Interpreter等工具实现LLM自动纠错。作者认为，手动测试代码至关重要，即使代码看起来完美且无错误，也不能保证其正确性。要避免LLM代码带来的问题，需要积极地测试代码，培养良好的手动QA技能。如果LLM生成的代码中频繁出现幻觉，可以尝试更换模型、学习如何使用上下文，或选择成熟的技术。作者还批评了那些认为审查LLM代码不如自己编写代码快的人，指出他们忽略了阅读、理解和审查他人代码的重要性，而LLM恰好可以帮助我们练习这些技能。

**要点总结**：

1.  **LLM代码幻觉是危害最小的错误**：当LLM生成不存在的方法或库时，程序会立即报错。这种错误很容易被发现和纠正，相比之下，LLM在其他领域产生的错误可能更难被察觉。

2.  **运行和测试代码至关重要**：不能仅仅依靠代码的外观或LLM的自信回答来判断其正确性。必须通过实际运行代码来验证其功能，手动测试代码是避免潜在问题的关键手段。

3.  **LLM无法取代软件专业人员**：尽管LLM可以生成看起来很棒的代码，但人工测试确保代码按预期运行仍然至关重要。编写代码后，需要运行来证明代码的有效性。

4.  **减少代码幻觉的技巧**：可以通过尝试不同的模型（如Claude 3.7 Sonnet、OpenAI的o3-mini-high和GPT-4o与代码解释器），向LLM提供相关库的示例代码，或选择长期使用的成熟库来减少幻觉。

5.  **提升代码审查能力**：审查LLM生成的代码是学习和提高代码阅读、理解和审查技能的绝佳途径。与其抱怨审查代码耗时，不如将其视为提升自身技能的机会。

