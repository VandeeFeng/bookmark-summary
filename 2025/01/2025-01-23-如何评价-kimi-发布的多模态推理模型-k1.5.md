# 如何评价 Kimi 发布的多模态推理模型 k1.5
- URL: https://www.fxzhihu.com/question/10114790245/answer/84028353434
- Added At: 2025-01-23 16:16:54
- [Link To Text](2025-01-23-如何评价-kimi-发布的多模态推理模型-k1.5_raw.md)

## Summary
**摘要**：
这篇文章主要讨论了Kimi团队在最新的多模态推理模型k1.5方面的工作，重点集中在它们解决模型推理过程中的思考和教训。主要涉及利用长文本输入（Context）和长文本输出（Chain of Thought）的模型优化，以及通过强化学习（RL）训练模型提升性能。文章也提及了团队在实现过程中遇到的关键问题和解决思路，特别是如何让模型通过强化学习进行自我反思和优化，减少犯错并提升长期价值。最终，作者强调了在持续推进AI研究和应用时，注重模型能否像人类一样进行自由思考的必要性。

**要点总结**：
- **技术聚焦**：Kimi团队发布k1.5模型，主要亮点在于对长Chain of Thought的有效性进行的深入探索和复现。
- **成本与性能**：在考量成本与性能的关系中，认识到长文本输出带来的挑战与价值，决定投入资源以提升模型的性能和精度。
- **跨领域启发**：通过观看特定的视频和学习重要文稿，从AlphaGo和LLM的学习过程，以及人类走棋的奖赏制定等多样来源中汲取灵感。
- **强化学习策略**：明确采用强化学习来训练模型，特别是In Context Reinforcement Learning（ICRL）方法，通过模仿人类思考过程，让模型自行搜索和自我校正。
- **长文本探索的技术难以度**：面对长文本输出条件下奖励信号难以准确定义的挑战，提出用原始的长文本作为历史信息，采用奖励信号从模型的自我反思过程中获取。
- **模型自身能力的长远作用**：强调模型自身能力的重要性，假以时日，技术发展中模Sus能实现的前途，对于模型能力的推动是必不可少的，而短期结构化工作可能被取代。
- **强化学习在长文本解释中的应用**：基于奖励信号和自反思，设计强化学习策略鼓励模型进行有用的探索，并通过基于竞争策略的训练手段提升模型在长文本解释方面的表现。
- **创造更多有价值的问题答案**：最终目标是在允许犯错的同时，优化模型性能，系统旨在为AI创造更多情境复杂、结构多元的真实问题，实现多种答案的自适应处理。
