# DeepSeek R1's recipe to replicate o1 and the future of reasoning LMs
- URL: https://www.interconnects.ai/p/deepseek-r1-recipe-for-o1
- Added At: 2025-01-28 09:10:26
- [Link To Text](2025-01-28-deepseek-r1's-recipe-to-replicate-o1-and-the-future-of-reasoning-lms_raw.md)

## Summary
**摘要**：
DeepSeek AI 在中国启动了其全面推理模型 R1 的发布，这项发布包含了以下多个方面：R1 是一个基于 4 站式、强化学习（RL）驱动的训练流程构建的顶级推理语言模型，通过 RL 软件许可证供企业及研究人员使用。R1 通过训练全推理模型 R1-Zero 的直接 RL-only 模型创建训练数据，另一个旨在使用监督微调（SFT）数据对应的 R1 模型的精调版本。DeepSeek 还发布了一份技术报告详细说明了其 RL 训练方法。R1 可通过 DeepThink 平台和新应用访问，这代表着推理模型研究领域的一个重大转折点。

**要点总结**：
1. **R1 的关键特性和技术优点**：R1 是通过重大 RL 流程，特别是强化学习反馈循环，来训练的一个旗舰级推理语言模型。该模型还采用了 RL 软件许可证，允许公司和研究人员基于模型的输出进行开发，加速推理语言模型的训练与应用。
2. **开放权重模型的后续开发**：DeepSeek AI 推出了 R1 模型的后续版本，包括 R1-Zero，这是一个应用 RL 直接从其 V3 基础模型训练的推理模型，以及从 R1 推导出的用于 R1 精调的监督微调数据集。
3. **定价差异的影响**：与 OpenAI 的 o1 模型进行了比较，特别指出 DeepSeek R1 的定价策略对比 OpenAI 的 o1 系列具有显著的经济优势，预计推理模型的市场价格会在未来经历激增。
4. **模型潜能的推测**：尽管具体应用领域尚不明确，但市场不断流传 OpenAI 的 o1-Pro 在处理复杂任务中的优势。提出 DeepSeek R1 使用拒绝采样（Rejection Sampling），并在后期引入了强化学习（RL）和淘汰不达标的数据点，进而提升了模型的通用能力。
5. **强化学习在模型训练中的趋势**：强化学习被用于改善模型的整体性能，特别强调了在 RL 和标准 RLHF 预偏置调整之间进行多奖模型训练，以提高模型的有用性、无害性和推理能力。

整体来看，DeepSeek R1 的发布标志着推理模型领域的最新进展，推动模型开发与应用进入到新阶段，其中涉及到强化学习技术的创新使用方式，以及内含的开放性质将加速这一领域的获取与合作。
