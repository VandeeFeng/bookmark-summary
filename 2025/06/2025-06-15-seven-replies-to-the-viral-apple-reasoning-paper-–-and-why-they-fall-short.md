# Seven replies to the viral Apple reasoning paper – and why they fall short
- URL: https://garymarcus.substack.com/p/seven-replies-to-the-viral-apple
- Added At: 2025-06-15 11:26:09
- [Link To Text](2025-06-15-seven-replies-to-the-viral-apple-reasoning-paper-–-and-why-they-fall-short_raw.md)

## Summary
**摘要**：
这篇文章讨论了苹果公司发布的一篇关于大语言模型推理局限性的论文引发的广泛争议和七种主要反驳观点。论文指出，当前的大语言模型在复杂推理任务（如汉诺塔问题）中存在显著缺陷，尤其在远离训练数据分布的情况下表现不佳。作者逐一反驳了七种常见的反对意见，包括人类也会犯错、输出令牌长度限制、作者身份质疑、模型规模提升、通过代码解决问题、案例数量不足以及已有认知等观点。文章强调，这些反驳都未能真正解决核心问题，即单纯扩大模型规模不足以实现通用人工智能（AGI），需要结合神经符号系统等其他方法。作者认为苹果论文和SalesForce的最新研究共同表明，当前技术尚无法可靠处理需要复杂推理和算法精度的任务。

**要点总结**：
1. **人类犯错不能成为模型缺陷的借口**：虽然人类在复杂问题和记忆需求上也有困难，但计算机的初衷正是为了弥补人类不足。大语言模型在汉诺塔等基本算法任务上的表现甚至不如现有专有系统，这与AGI应带来的进步背道而驰。关键在于模型无法在远离训练数据的情况下可靠执行算法。

2. **输出令牌长度限制仅是部分解释**：虽然模型输出长度限制可能导致无法完整表达复杂解决方案（如12盘汉诺塔），但这不能解释模型在255步内可解决的8盘汉诺塔上的失败。真正的通用智能系统不应受此限制。

3. **反驳"论文由实习生撰写"的偏见**：尽管第一作者是实习生，但团队包含多位资深研究者，且科学界第一作者往往是初学者的惯例。重要的是论文质量而非作者身份，类比基因图谱发明者本科期间的重要发现。

4. **单纯增加模型规模并非解决方案**：虽然更大模型可能表现更好，但无法预知何种规模足够解决特定问题。苹果研究表明模型可能在简单任务（6盘）表现良好却在稍复杂情况（8盘）崩溃，这种不可预测性令人担忧。

5. **需要神经符号结合的系统**：模型通过编写代码可能解决部分问题，这验证了神经符号AI的重要性。但仅靠下载代码缺乏概念理解，无法应对新问题或动态环境，正如数学考试考察的是理解而非计算结果。
