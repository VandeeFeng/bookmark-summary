# Drawing the Transformer Network from Scratch (Part 1)
- URL: https://towardsdatascience.com/drawing-the-transformer-network-from-scratch-part-1-9269ed9a2c5e
- Added At: 2024-11-04 06:50:17
- [Link To Text](2024-11-04-drawing-the-transformer-network-from-scratch-(part-1)_raw.md)

## Summary
**摘要**：
本文是探索Transformer架构中构建过程的系列第一部分（原本是「从零绘制变压器神经网络」系列的开头），此系列旨在以引人入胜的方式帮助读者形成清晰的Transformer概念模型。文章持续描绘它的各个组成部分，以从底部至顶部（Bottom-top fashion）的方式揭示整个架构，期望读者能轻松构建Transformer的“心理模型”。由谷歌团队于2017年提出的Transformer，因其“注意力机制”成为文献界中不可忽视的重要一员。

### 输入
Transformer接收到的输入是词汇序列，这些词汇以向量形式呈现。通常在自然语言处理任务中，使用词汇表（或字典）表示每个词汇，每个词汇分配一个唯一的索引，在实践中是词汇表中的至少10,000个不同索引。所有一热向量表示都有相同的欧几里得距离√2。为了减少维度并利用词汇向量的优势，文章将一热编码向量与命名为“嵌入矩阵”的矩阵相乘，得到更具体的“词汇嵌入”。

### 词汇嵌入
通过将一热编码向量与名为“嵌入矩阵”的矩阵相乘，减小向量的维度，以减少计算复杂度，并且能将具有相似含义的词放置在同一部分的向量空间中，这种方法显著降低了语言模型的存储需求。这不仅便于计算机处理，还优化了训练阶段的效率。

### 位置编码
位置信息不随输入序列顺序传递的问题由位置编码解决。在这个过程中，向每个输入向量添加一个向量，这种做法将顺序或绝对位置的信息注入输入序列中，使词汇正确地排列并降低时间复杂度。

### 关键与查询
向词向量执行调试乘以WQ和WK矩阵，以获得关键向量（Size 64）和查询向量（Size 64）。这一过程的目的是构建平均激励模型，允许Transformer在不同的词汇位置间学习依赖关系。

### 自注意力
执行所有可能的查询与关键向量的点积计算，点积将结果归一化用于作为权重因子。这些权重指示输入句子中不同位置处的单词间依赖程度，称为“自注意”。这意味着在特定位置考察关注的单词，以及忽略短篇文本处理中不相关或附近的单词。

### 规模与softmax
通过缩放权重因子，将它们除以8（关键向量的维度64的平方根），以确保训练阶段的稳定性。紧随缩放与softmax步骤，对局部分权因子执行了归一化过程，以便按比例考虑输入序列中的每个词汇频率。

### 值
从嵌入矩阵WV乘以词向量得到“值向量”，大小为64，关键在于自注意力结果的对比。接下来，我们利用权重因子对每个值向量进行加权，随后汇总所有加权值产生自注意力层的输出。

### DECORA
文章解释了Transformer的高级部分，包括多头自注意力模型，它使用了八个并行的注意力头部和一个额外的权重矩阵WO将推理与结果相加。这是一个关键步骤，确保模型同时关注不同表示子空间的信息。

需要补充的是，大多数谈及Transformer的文章都会忽视将其合并为一段文本，即约10,000个不同指示器实际声调阵列可以密集化为1024的序列层级，进行长度范围的有效压缩。维度削减有助于处理更长序列中的数据，同时通过额外步骤保持处理效率。此外，为确保训练时效率且稳定结果，不应忽视上述讨论的每个组件和步骤，使注意力机制在不同应用领域更加直观。

### 重点提炼**
1. **输入序列预处理**：Transformer接收词汇序列，并通过一热编码将其转换为低维度数字向量，用于便于计算和数据分析。
2. **嵌入与位置编码**：通过嵌入矩阵将原始向量化数据转换为表示更丰富特征的词语嵌入，并通过位置编码确保词汇的顺序信息。
3. **自注意力机制**：涵盖了利用点积和softmax函数调整权重的计算过程，以实现精准的自注意力，并通过权重编码依存关系，帮助模型识别相关的语言元素。
4. **多头自注意力**：整合了多个并行的注意力机制以提升模型性能和精细度，允许模型更加全面和精确地处理和转换输入信息。

以上内容为进一步理解Transformer在自然语言处理领域的应用提供了基础性理解，使复杂机制变得更加清晰易懂，进而能够在实际工作中应用于各种常见文本处理任务。
